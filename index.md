---
title: Zikang Xiong
permalink: /
layout: index 
---

# Introduction

I am a PhD student of [Computer Science Department](https://www.cs.purdue.edu/), [Purdue University](https://www.purdue.edu/). My advisor is [Suresh Jagannathan](https://www.cs.purdue.edu/homes/suresh/). I am working on attack/defense/assurance of deep learning, especially deep reinforcement learning, from the perspective of formal methods. I received my B.Eng. of Software Engineering from [University of Electronic Science and Technology of China](https://www.uestc.edu.cn/) in 2018.

[CV](https://www.cs.purdue.edu/homes/xiong84/res/cv/cv.pdf) updated at 2021-04-07.

# Contact
```sh
echo "emlrYW5neGlvbmdAZ21haWwuY29t" | base64 -d
```  

---

# Projects

## Verifiable Reinforcement Learning
Although deep neural networks have achieved promising performance in various tasks, they are generally used as black-box functions without any formal guarantee on their properties. For example, without formal analysis, it is unclear whether a neural-network-controlled drone, which is operated in a complex environment, will collide with the ground or not. This line of work provided verifiable safety guarantee for cyber-physical-systems (e.g., robots, UVA) trained with deep reinforcement learning.  

### Related publications:
*Scalable Synthesis of Verified Controllers in Deep Reinforcement Learning*  
**Zikang Xiong**, and Suresh Jagannathan.  
4th Workshop on Formal Methods for ML-Enabled Autonomous Systems \[[pdf](https://www.cs.purdue.edu/homes/xiong84/res/papers/CAV21.pdf)\]  

*An Inductive Synthesis Framework for Verifiable Reinforcement Learning.*   
He Zhu, **Zikang Xiong**, Stephen Magill and Suresh Jagannathan.    
PLDI 2019 \[[pdf](https://arxiv.org/pdf/1907.07273.pdf)\] \[[tool](https://github.com/caffett/VRL_CodeReview)\]  
<span style="color:red"> <em>Distinguished Paper</em> </span>


## Adversarial Attack & Defense of Deep Reinforcement Learning
Neural network controllers are not robust to adversarial attacks, which exposes them to great threats from malicious attackers. We aim to explore both attack and defense techniques for deep-neural-network controlled systems, thus providing more robust neural network controllers. 

### Related publication:
*Robustness to Adversarial Attacks in Learning-Enabled Controllers*  
**Zikang Xiong**, Joe Eappen, He Zhu and Suresh Jagannathan.  
Adaptive and Learning Agents Workshop
at AAMAS 2021 \[[pdf](https://www.cs.purdue.edu/homes/xiong84/res/papers/Adversarial20.pdf)\] \[[tool](https://hub.docker.com/repository/docker/caffett/neural_shield)\] \[[talk](https://www.youtube.com/watch?v=_52awZEp2iI)\]      

---

# Services
[ADHS 2021](https://sites.uclouvain.be/adhs21/) AEC, [CAV 2020](http://i-cav.org/2020/) AEC, [PLDI 2020](https://conf.researchr.org/home/pldi-2020) AEC

---

# Internship  
[Baidu Autonomous Driving Unit (Apollo)](https://apollo.auto/), Path Planning Optimization, 06/2021 - 09/2021.   

<!-- ---
# Others
ðŸ“š My recent paper reading [notes](https://xiong.zikang.me/blogs).    -->

